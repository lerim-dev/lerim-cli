# Lerim default configuration — shipped with the package.
# Override in ~/.lerim/config.toml (user) or <repo>/.lerim/config.toml (project).
# API keys come from environment variables only.

[data]
dir = "~/.lerim"

[memory]
scope = "project_fallback_global"   # project_fallback_global | project_only | global_only
project_dir_name = ".lerim"

[memory.decay]
enabled = true
decay_days = 180                    # days of no access before full decay
min_confidence_floor = 0.1          # decay never drops below this multiplier
archive_threshold = 0.2             # effective confidence below this → archive candidate
recent_access_grace_days = 30       # recently accessed memories skip archiving

[server]
host = "127.0.0.1"
port = 8765
poll_interval_minutes = 30
sync_interval_minutes = 10          # sync hot path interval
maintain_interval_minutes = 60      # maintain cold path interval
sync_window_days = 7
sync_max_sessions = 50
sync_max_workers = 4

[roles.lead]
provider = "openrouter"               # zai | openrouter | openai
model = "x-ai/grok-4.1-fast"
api_base = ""
fallback_models = []
timeout_seconds = 300
max_iterations = 24
openrouter_provider_order = []

[roles.explorer]
provider = "openrouter"
model = "x-ai/grok-4.1-fast"
api_base = ""
fallback_models = []
timeout_seconds = 180
max_iterations = 16
openrouter_provider_order = []

[roles.extract]
provider = "openrouter"
model = "x-ai/grok-4.1-fast"
sub_model = "x-ai/grok-4.1-fast"
api_base = ""
fallback_models = []
timeout_seconds = 180
max_iterations = 8
max_llm_calls = 12
openrouter_provider_order = []

[roles.summarize]
provider = "openrouter"
model = "x-ai/grok-4.1-fast"
sub_model = "x-ai/grok-4.1-fast"
api_base = ""
fallback_models = []
timeout_seconds = 180
max_iterations = 8
max_llm_calls = 12
openrouter_provider_order = []

[providers]
# Default API base URLs per provider.
# Override here to point all roles using that provider at a different endpoint.
# Per-role api_base (under [roles.*]) takes precedence over these defaults.
zai = "https://api.z.ai/api/paas/v4"
openai = "https://api.openai.com/v1"
openrouter = "https://openrouter.ai/api/v1"
ollama = "http://127.0.0.1:11434"

[tracing]
enabled = false                          # set true or LERIM_TRACING=1 to enable
include_httpx = false                    # capture raw HTTP request/response bodies
include_content = true                   # include prompt/completion text in spans

[agents]
# Map agent names to session directory paths.
# claude = "~/.claude/projects"
# codex = "~/.codex/sessions"
# cursor = "~/Library/Application Support/Cursor/User/globalStorage"
# opencode = "~/.local/share/opencode"

[projects]
# Map project short names to absolute host paths.
# my-project = "~/codes/my-project"

# --- Planned features (uncomment when implemented) ---
# [embeddings]
# provider = "local"
# model = "Alibaba-NLP/gte-modernbert-base"
# chunk_tokens = 800
# chunk_overlap_tokens = 200
# max_input_tokens = 8192

# [search]
# mode = "files"                       # files | fts | hybrid
# enable_fts = false
# enable_vectors = false
# enable_graph = true
# graph_depth = 1
# graph_export = false

# [paths]
# claude_dir = "~/.claude/projects"
# codex_dir = "~/.codex/sessions"
# opencode_dir = "~/.local/share/opencode"
# cursor_dir = "~/Library/Application Support/Cursor/User/globalStorage"
